{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script for NAMT-10 Hackathon\n",
    "**Learning outcome:** Train a classification model (one vs. all)\n",
    "<br> \n",
    "Therefore, use the CheXpertDataLoader from the first day and use it for training and evaluating your model.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Some challenges, you should keep in mind:\n",
    "1. What can you do to handle data imbalance? <font color=\"green\">*Oversampling minority class, weighted cross-entropy loss*</font> \n",
    "2. What can you do to learn from few samples and how can you prevent overfitting? <font color=\"green\">*Data augmentation, pre-trained models, early stopping*</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">*In baseline script only accuracy_score is imported from sklearn.metrics*</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import timm\n",
    "import timm.optim\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    classification_report)\n",
    "\n",
    "                 \n",
    "from key2med.data.loader import ColorCheXpertDataLoader\n",
    "from key2med.models.CBRTiny import CBRTiny\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger \n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some basic functions\n",
    "### Functions for saving / loading pickle objects\n",
    "During training we want to save the current epoch, training and validation loss, etc. to monitor our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj_pkl(path, obj):\n",
    "    with open( path, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj_pkl(path ):\n",
    "    with open( path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for evaluating your model\n",
    "You can either write your own evaluation metris and save it as a metric_dict or use pre-defined metrics from sklearn\n",
    "<br>\n",
    "**Which metrics are suitable to evaluate your model performance?**\n",
    "<br>\n",
    "<font color=\"green\">*In baseline script, only accuracy is used*</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics you want to use    \n",
    "def get_metric_dict(preds, y_true, y_pred):\n",
    " \n",
    "    metric_dict = {'Num_samples':len(y_true)}\n",
    "    metric_dict['Num'] = int(y_true.sum())\n",
    "    metric_dict['Acc'] = accuracy_score(y_true, y_pred)\n",
    "    metric_dict['bAcc'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metric_dict['Precision'] =  precision_score(y_true=y_true, y_pred=y_pred, zero_division = 0)\n",
    "    metric_dict['Recall'] =  recall_score(y_true=y_true, y_pred=y_pred, zero_division = 0)\n",
    "    metric_dict['F1'] =  f1_score(y_true=y_true, y_pred=y_pred, zero_division = 0)\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "    #Note that in binary classification, recall of the positive class is also known as “sensitivity”; recall of the negative class is “specificity”.\n",
    "    metric_dict['Sensitivity'] =  recall_score(y_true=y_true, y_pred=y_pred, zero_division = 0)\n",
    "    metric_dict['Specificity'] =  recall_score(y_true=~(y_true>0), y_pred=~(y_pred>0), zero_division = 0)\n",
    "    fpr, tpr, _ = roc_curve(y_true, preds[:,1])\n",
    "    metric_dict['AUC'] = auc(fpr, tpr)\n",
    "\n",
    "    return metric_dict\n",
    "\n",
    "# Define a evaluation function \n",
    "def eval_model(args, model, dataloader, dataset='valid'):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    y_preds = []\n",
    "    y_true = []\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(args['device'])\n",
    "        targets = targets.squeeze(dim=1).detach().cpu().numpy()\n",
    "        y_true += list(targets)\n",
    "        cur_preds = torch.nn.functional.softmax(model(inputs), dim=-1).detach().cpu().numpy()\n",
    "        preds += list(cur_preds)\n",
    "        y_preds += list( (cur_preds[:,1] > 0.5).astype(int))\n",
    "        \n",
    "    preds, y_preds, y_true =  np.asarray(preds), np.asarray(y_preds), np.asarray(y_true)\n",
    "    metric_dict = get_metric_dict(preds, y_true, y_preds)\n",
    "    with open(args['output_dir']+f'results_{dataset}_{args[\"class_positive\"]}.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(metric_dict, file, indent=2)\n",
    "    with open(args['output_dir']+f'results_{dataset}_{args[\"class_positive\"]}.pkl', 'wb') as file:\n",
    "        pickle.dump([metric_dict, y_true, y_preds, preds], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set default training settings\n",
    "For example define data path, batch size, number of epochs, etc.\n",
    "<br>\n",
    "Also specify here the class you are working with (Edema, Atelectasis, Cardiomegaly, Consolidation, Pleural Effusion)\n",
    "<br>\n",
    "<font color=\"green\">*In baseline script do_weight_loss_even, do_upsample, do_early_stopping and early_stopping_patience don't exist*</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42,\n",
       " 'data_dir': '/data/MEDICAL/datasets/CheXpert_small/CheXpert-v1.0-small',\n",
       " 'class_positive_str': 'Edema',\n",
       " 'class_positive': 5,\n",
       " 'channel_in': 3,\n",
       " 'freeze': True,\n",
       " 'model_to_load_dir': None,\n",
       " 'output_dir': '/home/student/hackathon/models',\n",
       " 'num_epochs': 10,\n",
       " 'max_steps': -1,\n",
       " 'do_train': True,\n",
       " 'max_dataloader_size': None,\n",
       " 'view': 'Frontal',\n",
       " 'batch_size': 24,\n",
       " 'num_workers': 4,\n",
       " 'lr': 0.001,\n",
       " 'wd': 1e-06,\n",
       " 'do_eval': True,\n",
       " 'eval_steps': 500,\n",
       " 'do_weight_loss_even': True,\n",
       " 'do_upsample': True,\n",
       " 'no_cuda': False,\n",
       " 'do_early_stopping': False,\n",
       " 'early_stopping_patience': 10,\n",
       " 'fp16': False,\n",
       " 'use_cache': True,\n",
       " 'in_memory': False,\n",
       " 'basePath': '/home/student/hackathon/models',\n",
       " 'device': device(type='cuda', index=0)}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name = 'Edema'\n",
    "\n",
    "label_to_index = { \n",
    "    'No Finding': 0,\n",
    "    'Enlarged Cardiom.': 1,\n",
    "    'Cardiomegaly':2,\n",
    "    'Lung Lesion':3,\n",
    "    'Lung Opacity':4,\n",
    "    'Edema':5,\n",
    "    'Consolidation':6,\n",
    "    'Pneumonia':7,\n",
    "    'Atelectasis':8,\n",
    "    'Pneumothorax':9,\n",
    "    'Pleural Effusion':10,\n",
    "    'Pleural Other':11,\n",
    "    'Fracture':12,\n",
    "    'Support Devices':13,\n",
    "    }\n",
    "\n",
    "args = {'seed': 42, # Set seed number if you want to reproduce results, else set it to None\n",
    "        'data_dir': '/data/MEDICAL/datasets/CheXpert_small/CheXpert-v1.0-small', # path to Chexpert data\n",
    "        'class_positive_str': class_name, # Set class name you are working on for one vs. all classification\n",
    "        'class_positive': label_to_index[class_name],\n",
    "        'channel_in': 3, # Number of input channels (3 because of color for pre-trained imagenet)\n",
    "        'freeze': True, # freeze model weights (if you use pre-trained model)\n",
    "        'model_to_load_dir': None, # model path, if you want to continue training\n",
    "        'output_dir': None,\n",
    "        'num_epochs': 10, # number of epochs for training the model\n",
    "        'max_steps': -1, # Total number of training steps. Low number for debugging, negative number for no limit.\n",
    "        'do_train': True, \n",
    "        'max_dataloader_size': None, # Set to low number for debugging. default = None (no limit) \n",
    "        'view': 'Frontal', # For DataLoader, do you want to load Frontal, Lateral or both views?\n",
    "        'batch_size': 24,\n",
    "        'num_workers': 4, # For DataLoader\n",
    "        'lr': 1e-3, # initial learning rate \n",
    "        'wd': 1e-6, # weight decay \n",
    "        'do_eval': True, # set to True if validation and test data should be evaluated after training\n",
    "        'eval_steps': 500, # Number of batches/steps to eval. Default 500.\n",
    "        'do_weight_loss_even': True, # Set to true if you want to use weighted loss function\n",
    "        'do_upsample': True,\n",
    "        'no_cuda': False,\n",
    "        'do_early_stopping': False, # Set to true, if you want to use early stopping \n",
    "        'early_stopping_patience': 10, # Stop training if after x steps validation loss has not increased \n",
    "        'fp16': False, # Set to True for mixed precision training \n",
    "        'use_cache': True,\n",
    "        'in_memory': False\n",
    "       }\n",
    "\n",
    "\n",
    "args['basePath'] = os.path.dirname(os.path.realpath(globals()['_dh'][0]))+os.sep\n",
    "\n",
    "if args['output_dir'] is None:\n",
    "    args['output_dir'] = f'{args[\"basePath\"]}training_output{os.sep}'\n",
    "else:\n",
    "    if args['output_dir'][-1] != os.sep: args['output_dir'] += os.sep\n",
    "\n",
    "if args['model_to_load_dir'] is not None:\n",
    "    if args['model_to_load_dir'][-1] != os.sep: args['model_to_load_dir'] += os.sep\n",
    "\n",
    "# set device (cuda if available or cpu)\n",
    "args['device'] = torch.device('cuda:0' if (torch.cuda.is_available() and not args['no_cuda']) else 'cpu')   \n",
    " \n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    filename=args['output_dir']+'train.log',\n",
    "    filemode='w',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed if args['seed'] is not None, to reproduce your results later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:08:50 ukb-rad-big-002 __main__[101738] INFO Applying seed 42\n"
     ]
    }
   ],
   "source": [
    "g = None # Generator for seed\n",
    "worker_init_fn = None\n",
    "if args['seed'] is not None:\n",
    "    logger.info(f'Applying seed {args[\"seed\"]}')\n",
    "    #https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    torch.manual_seed(args['seed'])\n",
    "    random.seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(args['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data\n",
    "Use the ColorChexpetDataLoader from key2med.data.loader\n",
    "<br>\n",
    "<font color=\"green\">*In baseline scirpt use_upsampling and upsample_labels don't exist*</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:08:50 ukb-rad-big-002 key2med.data.label_reader[101738] INFO Found labels in /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/train.csv: ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
      "Reading label csv file /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/train.csv: 5001it [00:00, 85196.48it/s]\n",
      "2022-03-16 15:08:50 ukb-rad-big-002 key2med.data.image_reader[101738] INFO Reading data from cache file /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/cache/ab1cb3d96608e48765b0cb9242ba987e16e2cd94b75c649ffbfa26ec251ade55\n",
      "2022-03-16 15:08:50 ukb-rad-big-002 key2med.data.image_reader[101738] INFO Found data of size (3, 224, 224) in file /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/cache/ab1cb3d96608e48765b0cb9242ba987e16e2cd94b75c649ffbfa26ec251ade55\n",
      "2022-03-16 15:08:52 ukb-rad-big-002 key2med.data.datasets[101738] INFO \n",
      "\t========== SPLIT train ==========:\n",
      "\tTotal images in split:  5,382\n",
      "\tTotal items in split:  5,382\n",
      "\tTotal imratio in split: 54.2%.\n",
      "\tEdema :   2,917 positive,   2,465 negative,       0 uncertain,   54.2% imratio.\n",
      "\n",
      "2022-03-16 15:08:52 ukb-rad-big-002 key2med.data.label_reader[101738] INFO Found labels in /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/train.csv: ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
      "Reading label csv file /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/train.csv: 5001it [00:00, 81304.05it/s]\n",
      "2022-03-16 15:08:52 ukb-rad-big-002 key2med.data.image_reader[101738] INFO Reading data from cache file /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/cache/ab1cb3d96608e48765b0cb9242ba987e16e2cd94b75c649ffbfa26ec251ade55\n",
      "2022-03-16 15:08:52 ukb-rad-big-002 key2med.data.image_reader[101738] INFO Found data of size (3, 224, 224) in file /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/cache/ab1cb3d96608e48765b0cb9242ba987e16e2cd94b75c649ffbfa26ec251ade55\n",
      "2022-03-16 15:08:53 ukb-rad-big-002 key2med.data.datasets[101738] INFO \n",
      "\t========== SPLIT valid ==========:\n",
      "\tTotal images in split:  399\n",
      "\tTotal items in split:  399\n",
      "\tTotal imratio in split: 32.8%.\n",
      "\tEdema :     131 positive,     268 negative,       0 uncertain,   32.8% imratio.\n",
      "\n",
      "2022-03-16 15:08:53 ukb-rad-big-002 key2med.data.label_reader[101738] INFO Found labels in /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/valid.csv: ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
      "Reading label csv file /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/valid.csv: 234it [00:00, 74095.36it/s]\n",
      "2022-03-16 15:08:53 ukb-rad-big-002 key2med.data.image_reader[101738] INFO Reading data from cache file /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/cache/35b73c135d187ee7562b3dbc4c9de1d8f30a29545fa403be1161b9a6b4f2e42e\n",
      "2022-03-16 15:08:53 ukb-rad-big-002 key2med.data.image_reader[101738] INFO Found data of size (3, 224, 224) in file /media/admin_ml/Volume/2022_Namt/CheXpert-v1.0-small/cache/35b73c135d187ee7562b3dbc4c9de1d8f30a29545fa403be1161b9a6b4f2e42e\n",
      "2022-03-16 15:08:53 ukb-rad-big-002 key2med.data.datasets[101738] INFO \n",
      "\t========== SPLIT test ==========:\n",
      "\tTotal images in split:  202\n",
      "\tTotal items in split:  202\n",
      "\tTotal imratio in split: 20.8%.\n",
      "\tEdema :      42 positive,     160 negative,       0 uncertain,   20.8% imratio.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,382 samples for training\n",
      "399 samples for validation\n",
      "202 samples for testing\n"
     ]
    }
   ],
   "source": [
    "dataloader = ColorCheXpertDataLoader( #CheXpertDataLoader #for 1 ch\n",
    "    data_path=args['data_dir'],\n",
    "    batch_size=args['batch_size'],\n",
    "    img_resize=224,\n",
    "    split_config=\"train_valid_test\",\n",
    "    channels=args['channel_in'],\n",
    "    do_random_transform=True,\n",
    "    use_cache=args['use_cache'],\n",
    "    in_memory=args['in_memory'],\n",
    "    max_size=args['max_dataloader_size'],\n",
    "    upsample_label =args['class_positive_str'] if args['do_upsample'] else None,\n",
    "    plot_stats=False,\n",
    "    n_workers=args['num_workers'],\n",
    "    valid_views = [args['view']], \n",
    "    label_filter = [args['class_positive']],\n",
    "    uncertain_to_one = [args['class_positive_str']],\n",
    "    uncertain_to_zero = [],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model you want to use for training\n",
    "You can use here your own implemented model or the CBRTiny model by Raghu et al. (https://arxiv.org/pdf/1902.07208.pdf), which is already implemented (from key2med.models.CBRTiny import CBRTiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBRTiny(num_classes=2, channel_in=args['channel_in']).to(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to use the timm library, where many well-known models are already implemented.\n",
    "<br>\n",
    "You can find the timm documentation here: https://fastai.github.io/timmdocs/\n",
    "<br>\n",
    "You can call up for example all existing pretrained efficient models in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17,\n",
       " ['efficientnet_b0',\n",
       "  'efficientnet_b1',\n",
       "  'efficientnet_b1_pruned',\n",
       "  'efficientnet_b2',\n",
       "  'efficientnet_b2_pruned'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avail_pretrained_models = timm.list_models('eff*',pretrained=True)\n",
    "# List number of all found models and the first five models\n",
    "len(avail_pretrained_models), avail_pretrained_models[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use here for example the efficientnetb0 model, pretrained on ImageNet (Set num_classes to 2 for one vs. all classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:08:53 ukb-rad-big-002 timm.models.helpers[101738] INFO Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b0_ra-3dd342df.pth)\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('efficientnet_b0', num_classes=2, in_chans=args['channel_in'], pretrained=True).to(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already startet the training, you can load the last checkpoint in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['model_to_load_dir'] is not None:\n",
    "    checkpoint = torch.load(osp.join(args['model_to_load_dir'], 'best_model.pth'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">**What can you do to prevent overfitting?**\n",
    "<br>\n",
    "If you use a pre-trained model, you can for example only train the classification layer and freeze all other model weights</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:08:53 ukb-rad-big-002 __main__[101738] INFO Freezing model\n"
     ]
    }
   ],
   "source": [
    "if args['freeze']:\n",
    "    logger.info(f'Freezing model')\n",
    "    args['output_dir'] += 'frozen'+os.sep\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name:    param.requires_grad = False\n",
    "else:\n",
    "    args['output_dir'] += 'unfrozen'+os.sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer and learning rate scheduler\n",
    "With timm library you can also use many pred-defined optimizers.\n",
    "<br>\n",
    "List all available optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdaBelief',\n",
       " 'Adafactor',\n",
       " 'Adahessian',\n",
       " 'AdamP',\n",
       " 'AdamW',\n",
       " 'Lamb',\n",
       " 'Lars',\n",
       " 'MADGRAD',\n",
       " 'Nadam',\n",
       " 'NvNovoGrad',\n",
       " 'RAdam',\n",
       " 'RMSpropTF',\n",
       " 'SGDP']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cls_name for cls_name, cls_obj in inspect.getmembers(timm.optim) if inspect.isclass(cls_obj) if cls_name !='Lookahead']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use for example the *AdamP* Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = timm.optim.create_optimizer_v2(model,\n",
    "                                           opt='AdamP',\n",
    "                                           lr=args['lr'],\n",
    "                                           weight_decay=args['wd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a learning rate scheduler using pytorch (for more infos https://pytorch.org/docs/stable/optim.html)\n",
    "<br>\n",
    "We can use for example the One Cylce Learning Rate Scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = args['num_epochs']*len(dataloader.train) if args['max_steps']<0 else args['max_steps'] #for Debug !\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=args['lr'],\n",
    "                                                total_steps=num_steps)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tensorboard for monitoring your model performance\n",
    "To do this, you need to set up a *SummaryWriter* that stores the current epoch count, current learning rate, training and validation loss, and current time.\n",
    "<br>\n",
    "Also, we store everything in the dictionary *writer_dict*, so you can create your own plots at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(args['output_dir']).mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "writer = SummaryWriter(args['output_dir']+os.sep+'runs')\n",
    "writer_dict = {\n",
    "                'epochs': [],#np.zeros(howOftenValid*howOftenRepeat,dtype=int),\n",
    "                'lr': [], #np.zeros(howOftenValid*howOftenRepeat),\n",
    "                'loss_train': [],#np.zeros(howOftenValid*howOftenRepeat),\n",
    "                'loss_valid': [],#np.zeros(howOftenValid*howOftenRepeat),\n",
    "                'walltime': [],#np.zeros(howOftenValid*howOftenRepeat)\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function\n",
    "You can for example use the cross entropy loss for a classification task\n",
    "<br>\n",
    "<br>\n",
    "<font color=\"green\">**What can you do here for handling class imbalance?**\n",
    "<br>\n",
    "For handling class imbalance, you can for example use a weighted cross entropy loss.\n",
    "<br>\n",
    "You can finde more details on the implementation here: https://discuss.pytorch.org/t/weights-in-weighted-loss-nn-crossentropyloss/69514/2</font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:08:54 ukb-rad-big-002 __main__[101738] INFO LOSS WEIGHTS: tensor([0.5420, 0.4580], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights for weighted cross entropy loss\n",
    "args['loss_weights'] = [1, 1]\n",
    "if args['do_weight_loss_even']:\n",
    "     #https://discuss.pytorch.org/t/weights-in-weighted-loss-nn-crossentropyloss/69514/2\n",
    "    num_total = len(dataloader.train.dataset.all_labels)\n",
    "    num_pos = sum(dataloader.train.dataset.all_labels)\n",
    "    num_neg = num_total-num_pos\n",
    "    args['loss_weights'] = [1 - (x / num_total) for x in [ num_neg, num_pos ] ]\n",
    "    \n",
    "if args['fp16']:\n",
    "    args['loss_weights'] = torch.tensor(args['loss_weights']).half().cuda()\n",
    "else: \n",
    "    args['loss_weights'] = torch.tensor(args['loss_weights']).float().cuda()\n",
    "    \n",
    "logger.info(f'LOSS WEIGHTS: {args[\"loss_weights\"]}')\n",
    "\n",
    "# Define cross entropy loss\n",
    "loss_function = torch.nn.CrossEntropyLoss(weight=args['loss_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write training loop\n",
    "\n",
    "<font color=\"green\">**What can you do here to prevent overfitting?**\n",
    "<br>\n",
    "For example, you can use early stopping or only save the best model found during validation and use it for inference.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:08:54 ukb-rad-big-002 __main__[101738] INFO Start Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79163067fe344710ae2d7d72b1b1f1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec3307b7e9644f8837247bab7501de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m writer_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(epoch)\n\u001b[1;32m     17\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutils/epochs\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch, steps) \u001b[38;5;66;03m# for tensorboard\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader\u001b[38;5;241m.\u001b[39mtrain, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     20\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m     steps_since_last_eval \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/tqdm/notebook.py:257\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m():\n\u001b[1;32m    258\u001b[0m             \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    259\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1207\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1173\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1173\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1175\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1014\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# If you want to perform mixed precision training \n",
    "scaler = None\n",
    "if args['fp16']:\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "best_loss = np.inf # for find best model on validation set\n",
    "eval_steps_since_last_better_model = 0 # for find best model on validation set \n",
    "if args['do_train']:\n",
    "\n",
    "    model.train()\n",
    "    steps=0\n",
    "    steps_since_last_eval=0\n",
    "    logger.info('Start Training')\n",
    "    for epoch in tqdm(range(args['num_epochs'])):\n",
    "\n",
    "        writer_dict['epochs'].append(epoch)\n",
    "        writer.add_scalar('utils/epochs', epoch, steps) # for tensorboard\n",
    "\n",
    "        for batch in tqdm(dataloader.train, leave=False):\n",
    "            steps += 1\n",
    "            steps_since_last_eval +=1\n",
    "\n",
    "            if steps > num_steps: break\n",
    "\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(args['device'])\n",
    "            targets = targets.squeeze(dim=1).long().to(args['device'])\n",
    "         \n",
    "\n",
    "     \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if args['fp16']:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(inputs)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()    \n",
    "            scheduler.step()\n",
    "            \n",
    "            writer_dict['walltime'].append( time.time() )\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            writer_dict['lr'].append(lr)\n",
    "            writer.add_scalar('utils/lr', lr, steps)\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "            writer_dict['loss_train'].append(loss)\n",
    "            writer.add_scalar('loss/train', loss, steps)\n",
    "\n",
    "            if steps_since_last_eval >= args['eval_steps']:\n",
    "                steps_since_last_eval = 0\n",
    "                if dataloader.validate is not None:\n",
    "                    model.eval()\n",
    "                    mean_loss = 0\n",
    "                    for batch in dataloader.validate:\n",
    "                        inputs, targets = batch\n",
    "                        inputs = inputs.to(args['device'])\n",
    "                        targets = targets.squeeze(dim=1).long().to(args['device'])\n",
    "                        if args['fp16']:\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                outputs = model(inputs)\n",
    "                        else:\n",
    "                            outputs = model(inputs)\n",
    "                        mean_loss += loss_function(outputs, targets).detach().cpu().numpy()\n",
    "\n",
    "                    mean_loss /= len(dataloader.validate)\n",
    "                    writer_dict['loss_valid'].append(mean_loss)\n",
    "                    writer.add_scalar('loss/valid', mean_loss, steps) # for tensorboard\n",
    "                    \n",
    "                    # Save best model\n",
    "                    model.train()\n",
    "                    if mean_loss < best_loss:\n",
    "\n",
    "                        best_loss = mean_loss\n",
    "                        eval_steps_since_last_better_model = 0\n",
    "\n",
    "                        with open( args['output_dir']+'best_loss.txt', 'w') as file:\n",
    "                            print( f'loss {best_loss}\\n step {steps}', file=file )\n",
    "\n",
    "                        torch.save({\n",
    "                                    'step': steps,\n",
    "                                    'model_state_dict': model.state_dict(),\n",
    "                                    'optimizer_state_dict': optimizer.state_dict()\n",
    "                                   }, args['output_dir']+'best_model.pth')\n",
    "                    else:\n",
    "                        # validation loss is higher than the best loss find in previous iterations \n",
    "                        # count steps for early stopping\n",
    "                        eval_steps_since_last_better_model += 1 \n",
    "                        \n",
    "                if args['do_early_stopping']:\n",
    "                    if eval_steps_since_last_better_model >= args['early_stopping_patience']: break\n",
    "\n",
    "            if steps >= num_steps: break\n",
    "\n",
    "    with open( args['output_dir']+'last_loss.txt', 'w') as file:\n",
    "        print( f'loss {mean_loss}\\n step {steps}', file=file )\n",
    "\n",
    "    torch.save({\n",
    "                'step': steps,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "               }, args['output_dir']+'last_model.pth')\n",
    "\n",
    "    save_obj_pkl( args['output_dir']+'tensorboard_writer.pkl', writer_dict )       \n",
    "    writer.close()\n",
    "    logger.info(f'End of training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model\n",
    "If you only want to evualate your model on the validation and test set, you have to set args['do_eval'] to True.\n",
    "<br>\n",
    "Then the evaluation function will be called after the training is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['do_eval']:\n",
    "    if dataloader.validate is not None:\n",
    "        logger.info('Start evaluation valid')\n",
    "        eval_model(args, model, dataloader.validate, dataset='valid')\n",
    "\n",
    "    if dataloader.test is not None:\n",
    "        logger.info('Start evaluation test')\n",
    "        eval_model(args, model, dataloader.test, dataset='test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
