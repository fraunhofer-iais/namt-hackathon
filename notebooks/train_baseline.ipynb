{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script for NAMT-10 Hackathon\n",
    "**Learning outcome:** Train a classification model (one vs. all)\n",
    "<br> \n",
    "Therefore, use the CheXpertDataLoader from the first day and use it for training and evaluating your model.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Some challenges, you should keep in mind:\n",
    "1. What can you do to handle data imbalance? \n",
    "2. What can you do to learn from few samples and how can you prevent overfitting? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import timm\n",
    "import timm.optim\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score)\n",
    "\n",
    "# import DataLoader\n",
    "from ... import ... \n",
    "# import model, e.g.\n",
    "from key2med.models.CBRTiny import CBRTiny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger \n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some basic functions\n",
    "### Functions for saving / loading pickle objects\n",
    "During training we want to save the current epoch, training and validation loss, etc. to monitor our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj_pkl(path, obj):\n",
    "   ...\n",
    "\n",
    "def load_obj_pkl(path ):\n",
    "   ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for evaluating your model\n",
    "You can either write your own evaluation metris and save it as a metric_dict or use pre-defined metrics from sklearn\n",
    "<br>\n",
    "**Which metrics are suitable to evaluate your model performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function \n",
    "def eval_model(args, model, dataloader, dataset='valid'):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    y_preds = []\n",
    "    y_true = []\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch\n",
    "        ...\n",
    "        y_true += ...\n",
    "        preds += ...\n",
    "        y_preds += ...\n",
    "        \n",
    "    metric_dict = {}\n",
    "    metric_dict['Acc'] = accuracy_score(y_true,_y_pred)\n",
    "    ... # Add more metrics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training settings \n",
    "For example define data path, batch size, number of epochs, etc.\n",
    "<br>\n",
    "Also specify here the class you are working with (Edema, Atelectasis, Cardiomegaly, Consolidation, Pleural Effusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set default settings\n",
    "You can define settings here, which you will need later on for reading the dataset and training your model, like number of epochs, learning rate, data directory and the class you are working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_dir': '/data/MEDICAL/datasets/CheXpert_small/CheXpert-v1.0-small',\n",
       " 'class_positive': 'Edema',\n",
       " 'num_epochs': 10,\n",
       " 'lr': 0.001}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {'data_dir': '/data/MEDICAL/datasets/CheXpert_small/CheXpert-v1.0-small', # path to Chexpert data\n",
    "        'class_positive': 'Edema', # Set class name you are working on for one vs. all classification\n",
    "        'num_epochs': 10, # number of epochs for training the model\n",
    "        'lr': 1e-3, # initial learning rate \n",
    "        # ...\n",
    "       }\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    filename=args['output_dir']+'train.log',\n",
    "    filemode='w',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data\n",
    "Use your dataloader from Hackathon#1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = CheXpertDataLoader(\n",
    "        data_path=args['data_dir'], \n",
    "        splits=\"train_valid_test\",\n",
    "        ...\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model you want to use for training\n",
    "You can use here your own implemented model or the CBRTiny model by Raghu et al. (https://arxiv.org/pdf/1902.07208.pdf), which is already implemented (from key2med.models.CBRTiny import CBRTiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBRTiny(num_classes=2, channel_in=args['channel_in']).to(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to use the timm library, where many well-known models are already implemented.\n",
    "<br>\n",
    "You can find the timm documentation here: https://fastai.github.io/timmdocs/\n",
    "<br>\n",
    "You can call up for example all existing pretrained efficient models in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail_pretrained_models = timm.list_models('eff*',pretrained=True)\n",
    "# List number of all found models and the first five models\n",
    "len(avail_pretrained_models), avail_pretrained_models[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use here for example the efficientnetb0 model, pretrained on ImageNet (Set num_classes to 2 for one vs. all classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('efficientnet_b0', num_classes=2, in_chans=args['channel_in'], pretrained=True).to(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already startet the training, you can load the last checkpoint in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['model_to_load_dir'] is not None:\n",
    "    checkpoint = torch.load(osp.join(args['model_to_load_dir'], 'best_model.pth'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer and learning rate scheduler\n",
    "With timm library you can also use many pred-defined optimizers.\n",
    "<br>\n",
    "List all available optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[cls_name for cls_name, cls_obj in inspect.getmembers(timm.optim) if inspect.isclass(cls_obj) if cls_name !='Lookahead']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use for example the *AdamP* Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = timm.optim.create_optimizer_v2(model,\n",
    "                                           opt='AdamP',\n",
    "                                           lr=args['lr'],\n",
    "                                           weight_decay=args['wd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should define a learning rate scheduler. You can for example use a learning rate scheduler from pytorch (for more infos https://pytorch.org/docs/stable/optim.html) or define your own function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(...):\n",
    "    ...\n",
    "\n",
    "# or:\n",
    "scheduler = torch.optim.lr_scheduler..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tensorboard for monitoring your model performance\n",
    "To do this, you need to set up a *SummaryWriter* that stores the current epoch count, current learning rate, training and validation loss, and current time.\n",
    "<br>\n",
    "Also, we store everything in the dictionary *writer_dict*, so you can create your own plots at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = '...'\n",
    "writer = SummaryWriter(outputPath+os.sep+'runs')\n",
    "writer_dict = {\n",
    "                'epochs': [],#np.zeros(howOftenValid*howOftenRepeat,dtype=int),\n",
    "                'lr': [], #np.zeros(howOftenValid*howOftenRepeat),\n",
    "                'loss_train': [],#np.zeros(howOftenValid*howOftenRepeat),\n",
    "                'loss_valid': [],#np.zeros(howOftenValid*howOftenRepeat),\n",
    "                'walltime': [],#np.zeros(howOftenValid*howOftenRepeat)\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function\n",
    "You can for example use the cross entropy loss for a classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross entropy loss\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "steps=0\n",
    "logger.info('Start Training')\n",
    "for epoch in tqdm(range(args['num_epochs'])):\n",
    "\n",
    "    writer_dict['epochs'].append(epoch)\n",
    "    writer.add_scalar('utils/epochs', epoch, steps) # for tensorboard\n",
    "\n",
    "    for batch in tqdm(dataloader.train, leave=False):\n",
    "        steps += 1\n",
    "        steps_since_last_eval +=1\n",
    "\n",
    "        inputs, targets = batch\n",
    "        ...  \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # append to writer (for Tensorflow) and writer_dict for monitoring your model performance\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        writer_dict['lr'].append(lr)\n",
    "        writer.add_scalar('utils/lr', lr, steps)\n",
    "        ...\n",
    "        \n",
    "        # evaluate your model on validation set\n",
    "        if dataloader.validate is not None:\n",
    "            model.eval()\n",
    "            mean_loss = 0\n",
    "            \n",
    "            for batch in dataloader.validate:\n",
    "               ...\n",
    "           \n",
    "            writer_dict['loss_valid'].append(mean_loss)\n",
    "            writer.add_scalar('loss/valid', mean_loss, steps) # for tensorboard\n",
    "\n",
    "     \n",
    "\n",
    "# Save your model:\n",
    "torch.save(...)\n",
    "\n",
    "writer.close()\n",
    "logger.info(f'End of training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model\n",
    "If you want to evaluate your model on the validation and test sets, use the eval_model function defined at the beginning of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataloader.validate is not None:\n",
    "    logger.info('Start evaluation valid')\n",
    "    eval_model(args, model, dataloader.validate, dataset='valid')\n",
    "\n",
    "if dataloader.test is not None:\n",
    "    logger.info('Start evaluation test')\n",
    "    eval_model(args, model, dataloader.test, dataset='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
